{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.expanduser(\"~/Projects/zimmer\"))\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "npr.seed(1234)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn\n",
    "seaborn.set_style(\"white\")\n",
    "seaborn.set_context(\"paper\")\n",
    "plt.ion()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "from zimmer.io import load_nichols_data\n",
    "import zimmer.plotting as zplt\n",
    "from zimmer.observations import HierarchicalAutoRegressiveObservations,  HierarchicalRobustAutoRegressiveObservations\n",
    "from zimmer.transitions import HierarchicalStationaryTransitions, HierarchicalRecurrentTransitions, HierarchicalRecurrentOnlyTransitions, \\\n",
    "    HierarchicalNeuralNetworkRecurrentTransitions, GroupRecurrentTransitions, ElaborateGroupRecurrentTransitions\n",
    "from zimmer.util import cached\n",
    "\n",
    "from ssm.models import HMM\n",
    "from ssm.core import _HMM\n",
    "from ssm.init_state_distns import InitialStateDistribution\n",
    "from ssm.transitions import RecurrentTransitions, InputDrivenTransitions, StationaryTransitions, \\\n",
    "    NeuralNetworkRecurrentTransitions, RecurrentOnlyTransitions\n",
    "from ssm.observations import RobustAutoRegressiveObservations\n",
    "\n",
    "from ssm.util import find_permutation, compute_state_overlap\n",
    "from ssm.preprocessing import pca_with_imputation, trend_filter, standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify hyperparameters\n",
    "D = 10   # dimensionality of continuous latent states\n",
    "M = 0    # dimensionality of input\n",
    "results_dir = \"results/nichols/2018-12-31/D{}\".format(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\"n2_1_prelet\", \n",
    "          \"n2_2_let\",\n",
    "          \"npr1_1_prelet\",\n",
    "          \"npr1_2_let\"]\n",
    "\n",
    "nice_groups = [\"N2\\n(prelethargic)\", \"N2\\n(lethargic)\", \"npr1\\n(prelethargic)\", \"npr1\\n(lethargic)\"]\n",
    "\n",
    "worms_and_groups = [(i, \"n2_1_prelet\") for i in range(11)] + \\\n",
    "                   [(i, \"n2_2_let\") for i in range(12)] + \\\n",
    "                   [(i, \"npr1_1_prelet\") for i in range(10)] + \\\n",
    "                   [(i, \"npr1_2_let\") for i in range(11)]\n",
    "worm_names = [\"{} worm {}\".format(group, i) for (i, group) in worms_and_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only including named neurons.\n",
      "73 neurons across all 44 worms\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "ys, ms, us, z_trues, z_true_key, neuron_names = load_nichols_data(worms_and_groups, worm_names, include_unnamed=False, signal=\"dff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = [trend_filter(y) for y in ys]\n",
    "# ys = [standardize(y, m) for y, m in zip(ys, ms)]\n",
    "K_true = len(z_true_key)\n",
    "N = ys[0].shape[1]\n",
    "W = len(ys)\n",
    "Ts = [y.shape[0] for y in ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "windows blue : Quiescence\n",
      "red : Forward\n",
      "amber : Reversal\n",
      "faded green : Ventral turn\n",
      "dusty purple : Dorsal turn\n",
      "orange : undefined turn\n"
     ]
    }
   ],
   "source": [
    "for cname, zname in zip(zplt.color_names, z_true_key):\n",
    "    print(cname, \":\", zname )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/nichols/2018-12-31/D10'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scott/anaconda3/lib/python3.6/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator PCA from version 0.19.1 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Run PCA to get a 3d projection of the data\n",
    "from ssm.preprocessing import pca_with_imputation\n",
    "_pca = cached(results_dir, \"pca\")(pca_with_imputation)\n",
    "pca, xs = _pca(D, ys, ms, num_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train/test/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = 250\n",
    "train_frac = 0.7\n",
    "val_frac = 0.15\n",
    "all_ys = []\n",
    "all_xs = []\n",
    "all_ms = []\n",
    "all_us = []\n",
    "all_gs = []\n",
    "all_tags = []\n",
    "all_z_trues = []\n",
    "all_choices = []\n",
    "for tag, (y, x, m, u, tag, ztr) in enumerate(zip(ys, xs, ms, us, worms_and_groups, z_trues)):\n",
    "    T = y.shape[0]\n",
    "    C = 0\n",
    "    for start in range(0, T, chunk):\n",
    "        stop = min(start+chunk, T)\n",
    "        all_ys.append(y[start:stop])\n",
    "        all_xs.append(x[start:stop])\n",
    "        all_ms.append(m[start:stop])\n",
    "        all_us.append(u[start:stop])\n",
    "        all_z_trues.append(ztr[start:stop])\n",
    "        all_tags.append(tag)\n",
    "        C += 1\n",
    "        \n",
    "    # assign some of the data to train, val, and test\n",
    "    choices = -1 * np.ones(C)\n",
    "    choices[:int(train_frac * C)] = 0\n",
    "    choices[int(train_frac * C):int((train_frac + val_frac) * C)] = 1\n",
    "    choices[int((train_frac + val_frac) * C):] = 2\n",
    "    choices = choices[np.random.permutation(C)]\n",
    "    all_choices.append(choices)\n",
    "\n",
    "all_choices = np.concatenate(all_choices)\n",
    "get = lambda arr, chc: [x for x, c in zip(arr, all_choices) if c == chc]\n",
    "\n",
    "train_ys = get(all_ys, 0)\n",
    "train_xs = get(all_xs, 0)\n",
    "train_ms = get(all_ms, 0)\n",
    "train_us = get(all_us, 0)\n",
    "train_zs = get(all_z_trues, 0)\n",
    "# train_tags = get(all_gs, 0)\n",
    "train_tags = get(all_tags, 0)\n",
    "\n",
    "val_ys = get(all_ys, 1)\n",
    "val_xs = get(all_xs, 1)\n",
    "val_ms = get(all_ms, 1)\n",
    "val_us = get(all_us, 1)\n",
    "val_zs = get(all_z_trues, 1)\n",
    "# val_tags = get(all_gs, 1)\n",
    "val_tags = get(all_tags, 1)\n",
    "\n",
    "test_ys = get(all_ys, 2)\n",
    "test_xs = get(all_xs, 2)\n",
    "test_ms = get(all_ms, 2)\n",
    "test_us = get(all_us, 2)\n",
    "test_zs = get(all_z_trues, 2)\n",
    "# test_tags = get(all_gs, 2)\n",
    "test_tags = get(all_tags, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_train:  986680\n",
      "D_val:    219100\n",
      "D_test:   219340\n"
     ]
    }
   ],
   "source": [
    "# Compute number of training, validation, and test entries\n",
    "D_train = sum([x.size for x in train_xs])\n",
    "D_val = sum([x.size for x in val_xs])\n",
    "D_test = sum([x.size for x in test_xs])\n",
    "\n",
    "print(\"D_train: \", D_train)\n",
    "print(\"D_val:   \", D_val)\n",
    "print(\"D_test:  \", D_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit an ARHMM to the continuous latent states, sweeping over number of discrete latent states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ks = np.arange(2, 21, step=2)\n",
    "# etas = [1e0, 1e-1, 1e-2, 1e-3, 1e-4]\n",
    "# transitionss = [\"recurrent\"]\n",
    "# observationss = [\"robust_ar\"]\n",
    "Ks = np.arange(12, 13, step=2)\n",
    "etas = [1e-3]\n",
    "transitionss = [\"recurrent\"]\n",
    "observationss = [\"robust_ar\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fit_hmm(K, transitions, observations):\n",
    "    hmm = HMM(K, D, M, transitions=transitions, observations=observations)\n",
    "    lps = hmm.fit(train_xs, method=\"em\", num_em_iters=250)\n",
    "    val_ll = hmm.log_likelihood(val_xs)\n",
    "    return hmm, lps, val_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting  hmm_recurrent_robust_ar_K12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scott/anaconda3/lib/python3.6/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.19.1 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "hmm_results = {}\n",
    "for K in Ks:\n",
    "    for transitions in transitionss:\n",
    "        for observations in observationss:\n",
    "            hmm_results_name = \"hmm_{}_{}_K{}\".format(transitions, observations, K)\n",
    "            fit = cached(results_dir, hmm_results_name)(_fit_hmm)\n",
    "            \n",
    "            print(\"Fitting \", hmm_results_name)\n",
    "            hmm_results[hmm_results_name] = fit(K, transitions, observations)\n",
    "            \n",
    "with open(os.path.join(results_dir, \"hmm_results.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(hmm_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit hierarchical HMMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fit_hierarchical_hmm(K, eta, transitions, observations, hmm):\n",
    "    # Construct the HMM components\n",
    "    init_state_distn = InitialStateDistribution(K, D, M)\n",
    "\n",
    "    transition_classes = dict(recurrent=ElaborateGroupRecurrentTransitions)\n",
    "    transition_distn = transition_classes[transitions](K, D, worms_and_groups, M, eta1=eta, eta2=1e-1)\n",
    "    assert transition_distn.T == len(worms_and_groups)\n",
    "    assert transition_distn.G == 4\n",
    "    \n",
    "    observation_classes = dict(ar=HierarchicalAutoRegressiveObservations, \n",
    "                               robust_ar=HierarchicalRobustAutoRegressiveObservations)\n",
    "    observation_distn = observation_classes[observations](K, D, worms_and_groups, M, eta=eta)\n",
    "    \n",
    "    # Construct the HMM\n",
    "    hhmm = _HMM(K, D, M, init_state_distn, transition_distn, observation_distn)\n",
    "\n",
    "    # Initialize with the standard HMM\n",
    "    hhmm.init_state_distn.params = copy.deepcopy(hmm.init_state_distn.params)\n",
    "    hhmm.transitions.initialize_from_standard(hmm.transitions)\n",
    "    hhmm.observations.initialize_from_standard(hmm.observations)\n",
    "\n",
    "    # Fit\n",
    "    lps = hhmm.fit(train_xs, tags=train_tags, method=\"em\", num_em_iters=250, initialize=False)\n",
    "    \n",
    "    # Validate\n",
    "    val_ll = hhmm.log_likelihood(val_xs, tags=val_tags)\n",
    "    return hhmm, lps, val_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting  hhmm_recurrent_robust_ar_K12_eta1e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f841227baa14b27bbbb241bff1b7d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scott/Projects/ssm/ssm/util.py:192: UserWarning: generalized_newton_studentst_dof fixed point grew beyond bounds [0.001,20].\n",
      "  \"bounds [{},{}].\".format(nu_min, nu_max))\n"
     ]
    }
   ],
   "source": [
    "hhmm_results = {}\n",
    "for K in Ks:\n",
    "    for eta in etas:\n",
    "        for transitions in transitionss:\n",
    "            for observations in observationss:\n",
    "                # Get the HMM results\n",
    "                hmm_results_name = \"hmm_{}_{}_K{}\".format(transitions, observations, K)\n",
    "                hmm, _, _ = hmm_results[hmm_results_name]\n",
    "\n",
    "                # Fit the Hierarchical HMM\n",
    "                hhmm_results_name = \"hhmm_{}_{}_K{}_eta{:1.0e}\".format(transitions, observations, K, eta)\n",
    "                fit = cached(results_dir, hhmm_results_name)(_fit_hierarchical_hmm)\n",
    "                print(\"Fitting \", hhmm_results_name)\n",
    "                hhmm_results[hhmm_results_name] = fit(K, eta, transitions, observations, hmm)\n",
    "            \n",
    "with open(os.path.join(results_dir, \"hhmm_results.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(hhmm_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and validation likelihoods\n",
    "\n",
    "# num_bars = len(transitions) * len(observations)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for transitions in transitionss:\n",
    "    for observations in observationss:\n",
    "        hmm_results_prefix = \"hmm_{}_{}_\".format(transitions, observations)\n",
    "        hmm_val_lls = [hmm_results[hmm_results_prefix + \"K{}\".format(K)][2] for K in Ks]\n",
    "        plt.plot(Ks, hmm_val_lls, '-ko', label=hmm_results_prefix)\n",
    "\n",
    "        for eta in etas:\n",
    "            hhmm_results_prefix = \"hhmm_{}_{}_\".format(transitions, observations)\n",
    "            hhmm_val_lls = [hhmm_results[hhmm_results_prefix + \"K{}_eta{:1.0e}\".format(K, eta)][2] for K in Ks]\n",
    "            plt.plot(Ks, hhmm_val_lls, '-o', label=hhmm_results_prefix + \" eta={:1.0e}\".format(eta))\n",
    "\n",
    "plt.xlabel(\"K\")\n",
    "plt.xticks(Ks)\n",
    "plt.ylabel(\"Validation LL\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the HHMM to the full train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_K = 12\n",
    "best_eta = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fit_to_all_data(model):\n",
    "    lps = model.fit(train_xs + val_xs, tags=train_tags + val_tags, method=\"em\", num_em_iters=100, initialize=False)\n",
    "    return model, lps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the HMMs\n",
    "best_results = {}\n",
    "for transitions in transitionss:\n",
    "    for observations in observationss:\n",
    "        hhmm_results_prefix = \"hhmm_{}_{}\".format(transitions, observations)\n",
    "        # hhmm_val_lls = [hhmm_results[hhmm_results_prefix + \"_K{}_eta{:1.0e}\".format(K, best_eta)][2] for K in Ks]\n",
    "        # best_K = Ks[np.argmax(hhmm_val_lls)]\n",
    "        \n",
    "        # Fit the best model    \n",
    "        results_name = \"best_\" + hhmm_results_prefix + \"_eta{:1.0e}\".format(best_eta)\n",
    "        print(\"Fitting \", results_name, \" with K = \", best_K)\n",
    "        fit = cached(results_dir, results_name)(_fit_to_all_data)\n",
    "        hhmm, _ = fit(hhmm_results[hhmm_results_prefix + \"_K{}_eta{:1.0e}\".format(best_K, best_eta)][0])\n",
    "        \n",
    "        # Compute the log likelihood of the test data\n",
    "        test_ll = hhmm.log_likelihood(test_xs, tags=test_tags)\n",
    "        best_results[results_name] = (hhmm, test_ll)\n",
    "        \n",
    "with open(os.path.join(results_dir, \"best_results.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(best_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhmm, _ = best_results[\"best_hhmm_recurrent_robust_ar_eta1e-03\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the inferred states\n",
    "z_infs = [hhmm.most_likely_states(x, tag=tag) for tag, x in zip(worms_and_groups, xs)]\n",
    "hhmm.permute(find_permutation(np.concatenate(z_trues), np.concatenate(z_infs)))\n",
    "z_infs = [hhmm.most_likely_states(x, tag=tag) for tag, x in zip(worms_and_groups, xs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 16))\n",
    "# lim = 6\n",
    "# w_to_plot=(0, 11, 23, 33)\n",
    "# for i,w in enumerate(w_to_plot):\n",
    "#     x = xs[w]\n",
    "#     z= z_infs[w]\n",
    "#     for d in range(1, 4):\n",
    "#         ax = plt.subplot(4, 4, (d-1) * 4 + i+1)\n",
    "#         zplt.plot_2d_continuous_states(x, z, xlims=(-lim, lim), ylims=(-lim, lim), inds=(0, d), ax=ax)\n",
    "#         plt.ylabel(\"PC {}\".format(d+1) if i == 0 else \"\")\n",
    "#         plt.title(worm_names[w])\n",
    "\n",
    "# plt.suptitle(\"Continuous Latent States (Inferred Labels)\")\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the overlap between inferred and true states\n",
    "K_zimmer = len(z_true_key)\n",
    "overlap = np.zeros((K_zimmer, hhmm.K), dtype=float)\n",
    "for ww in range(W):\n",
    "    for k1 in range(K_zimmer):\n",
    "        for k2 in range(hhmm.K):\n",
    "            overlap[k1, k2] += np.sum((z_trues[ww] == k1) & (z_infs[ww] == k2))\n",
    "\n",
    "# Normalize the overlap from all worms and plot\n",
    "overlap /= overlap.sum(1)[:, None]\n",
    "\n",
    "# Permute to make blocky as possible\n",
    "most_overlap = np.argmax(overlap, axis=0)\n",
    "perm_overlap = np.argsort(most_overlap)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "gs = GridSpec(11, 12)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[:10, 1:11])\n",
    "im = ax1.imshow(overlap[:, perm_overlap], vmin=0, vmax=1.0, cmap=\"Greys\", interpolation=\"nearest\", aspect=\"equal\")\n",
    "ax1.set_yticks([])\n",
    "ax1.set_xticks([])\n",
    "ax1.set_title(\"state overlap\")\n",
    "\n",
    "lax = fig.add_subplot(gs[:10, 0])\n",
    "lax.imshow(np.arange(K_zimmer)[:, None], cmap=zplt.default_cmap, \n",
    "              vmin=0, vmax=len(zplt.default_colors) - 1, aspect=1.5)\n",
    "\n",
    "lax.set_xticks([])\n",
    "lax.set_yticks(np.arange(K_zimmer))\n",
    "lax.set_yticklabels(z_true_key)\n",
    "\n",
    "bax = fig.add_subplot(gs[10, 1:11])\n",
    "bax.imshow(np.arange(hhmm.K)[None, perm_overlap], cmap=zplt.default_cmap,  aspect=\"auto\",\n",
    "           vmin=0, vmax=len(zplt.default_colors)-1)\n",
    "bax.set_xticks([])\n",
    "bax.set_yticks([])\n",
    "bax.set_xlabel(\"inferred state\")\n",
    "\n",
    "axcb = fig.add_subplot(gs[:10, 11])\n",
    "plt.colorbar(im, cax=axcb, ticks=[0, 0.5, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_model(name, model, pad=3, N_smpls=20):\n",
    "    # Get the inferred states\n",
    "    z_infs = [model.most_likely_states(x, tag=tag) for tag, x in zip(worms_and_groups, xs)]\n",
    "    model.permute(find_permutation(np.concatenate(z_trues), np.concatenate(z_infs)))\n",
    "    z_infs = [model.most_likely_states(x, tag=tag) for tag, x in zip(worms_and_groups, xs)]\n",
    "    \n",
    "    # Lower the noise\n",
    "    model_low_noise = copy.deepcopy(model)\n",
    "    model_low_noise.observations.inv_sigmas -= 4\n",
    "    \n",
    "    simulations = []\n",
    "\n",
    "    for g in range(4):\n",
    "        # Simulate N_smpls for this worm\n",
    "        model_simulations_g = []\n",
    "        for s in range(N_smpls):\n",
    "            print(\"Model \", name, \" Group \", g, \" sample \", s)\n",
    "            # Sample data\n",
    "            Tsmpl = Ts[g*12]\n",
    "            zpre, xpre = z_infs[g*12][-pad:], xs[g*12][-pad:]\n",
    "            zsmpl, xsmpl = model_low_noise.sample(Tsmpl, prefix=(zpre, xpre), tag=(0, groups[g]), with_noise=True)\n",
    "\n",
    "            zsmpl = np.concatenate((zpre, zsmpl))\n",
    "            xsmpl = np.concatenate((xpre, xsmpl))\n",
    "\n",
    "            # Truncate to stable region\n",
    "            unstable = np.arange(Tsmpl+pad)[np.any(abs(xsmpl) > 10, axis=1)]\n",
    "            T_stable = np.min(np.concatenate(([Tsmpl+pad], unstable)))\n",
    "            zsmpl = zsmpl[:T_stable]\n",
    "            xsmpl = xsmpl[:T_stable]\n",
    "\n",
    "            # Project into neural space\n",
    "            ysmpl = xsmpl.dot(pca.components_) + pca.mean_\n",
    "\n",
    "            # Append\n",
    "            model_simulations_g.append((zsmpl, xsmpl, ysmpl))\n",
    "            \n",
    "        # Append this worm\n",
    "        simulations.append(model_simulations_g)\n",
    "        \n",
    "    return simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simulations = {}\n",
    "for name, (model, _) in best_results.items(): \n",
    "    print(\"Simulating \", name)\n",
    "    _sim = cached(results_dir, name + \"_sim\")(simulate_model)\n",
    "    simulations[name] = _sim(name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for g in range(4):\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for z, x, y in simulations[\"best_hhmm_recurrent_robust_ar_eta1e-03\"][g][:2]:\n",
    "        zplt.plot_2d_continuous_states(x, z, ax=ax, xlims=(-6, 6), ylims=(-6, 6))\n",
    "#         plt.plot(x[:,0], x[:,1], color='k', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fit a model with inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2 = (us[0] - 13.6678) / 5.185\n",
    "do2 = np.concatenate(([0], np.diff(o2)))\n",
    "\n",
    "f = np.exp(-np.arange(3 * 10) / (3 * 3))\n",
    "f /= f.sum()\n",
    "f_do2 = np.convolve(do2, f, mode=\"full\")[:-len(f)]\n",
    "\n",
    "    \n",
    "plt.plot(o2)\n",
    "plt.plot(do2)\n",
    "plt.plot(f_do2)\n",
    "# plt.xlim(1000, 1200)\n",
    "plt.xlim(2000, 2200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do2.max(), do2.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert u into o2 and do2s\n",
    "# E[u] = 13.6678;  std[u] = 5.185\n",
    "def u_to_features(u, nlags=9):\n",
    "    o2 = (u - 13.6678) / 5.185\n",
    "    do2 = np.concatenate(([0], np.diff(o2)))\n",
    "    \n",
    "    # Also add the smoothed do2\n",
    "    f = np.exp(-np.arange(3 * 10) / (3 * 3))\n",
    "    f /= f.sum()\n",
    "    f_do2 = np.convolve(do2, f, mode=\"full\")[:-len(f)+1]\n",
    "    \n",
    "    features = [o2, f_do2]\n",
    "    return np.column_stack(features)\n",
    "\n",
    "all_inputs = [u_to_features(u) for u in us]\n",
    "train_inputs = [u_to_features(u) for u in train_us]\n",
    "val_inputs = [u_to_features(u) for u in val_us]\n",
    "test_inputs = [u_to_features(u) for u in test_us]\n",
    "M = test_inputs[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fit_hierarchical_hmm_with_inputs(M, K, transitions, observations, hmm):\n",
    "    # Construct the HMM components\n",
    "    init_state_distn = InitialStateDistribution(K, D, 0)\n",
    "\n",
    "    transition_classes = dict(recurrent=ElaborateGroupRecurrentTransitions)\n",
    "    transition_distn = transition_classes[transitions](K, D, worms_and_groups, 0, eta1=best_eta, eta2=1e0\n",
    "                                                      )\n",
    "    assert transition_distn.T == len(worms_and_groups)\n",
    "    assert transition_distn.G == 4\n",
    "    \n",
    "    observation_classes = dict(ar=HierarchicalAutoRegressiveObservations, \n",
    "                               robust_ar=HierarchicalRobustAutoRegressiveObservations)\n",
    "    observation_distn = observation_classes[observations](K, D, worms_and_groups, 0, eta=best_eta)\n",
    "    \n",
    "    # Construct the HMM\n",
    "    hhmm = _HMM(K, D, 0, init_state_distn, transition_distn, observation_distn)\n",
    "\n",
    "    # Initialize with the standard HMM\n",
    "    hhmm.init_state_distn.params = copy.deepcopy(hmm.init_state_distn.params)\n",
    "    hhmm.transitions.initialize_from_standard(hmm.transitions)\n",
    "    hhmm.observations.initialize_from_standard(hmm.observations)\n",
    "\n",
    "    # Override M and set the input parameters\n",
    "    Gt, Go = hhmm.transitions.G, hhmm.observations.G\n",
    "    hhmm.M = M\n",
    "\n",
    "    hhmm.transitions.M = M\n",
    "    hhmm.transitions.eta2 = 1\n",
    "    hhmm.transitions.shared_Ws = np.zeros((K, M))\n",
    "    hhmm.transitions.Ws = np.zeros((Gt, K, M))\n",
    "    \n",
    "    hhmm.observations.M = M\n",
    "    hhmm.observations.shared_Vs = np.zeros((K, D, M))\n",
    "    hhmm.observations.Vs = np.zeros((Go, K, D, M))\n",
    "    \n",
    "    # Fit\n",
    "    lps = hhmm.fit(train_xs, inputs=train_inputs, tags=train_tags, method=\"em\", num_em_iters=250, initialize=False)\n",
    "    \n",
    "    # Validate\n",
    "    val_ll = hhmm.log_likelihood(val_xs, inputs=val_inputs, tags=val_tags)\n",
    "    return hhmm, lps, val_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for K in Ks:\n",
    "    for transitions in transitionss:\n",
    "        for observations in observationss:\n",
    "            # Get the results of the HHMM without inputs\n",
    "            # base_results_name = \"hhmm_{}_{}_K{}_eta{:1.0e}\".format(transitions, observations, K, best_eta)\n",
    "            # base, _, _ = hhmm_results[base_results_name]\n",
    "            hmm_results_name = \"hmm_{}_{}_K{}\".format(transitions, observations, K)\n",
    "            hmm, _, _ = hmm_results[hmm_results_name]\n",
    "\n",
    "\n",
    "            # Fit the HHMM with inputs\n",
    "            hhmm_results_name = \"hhmm_{}_{}_K{}_eta{:1.0e}_inputs\".format(transitions, observations, K, best_eta)\n",
    "            fit = cached(results_dir, hhmm_results_name)(_fit_hierarchical_hmm_with_inputs)\n",
    "            print(\"Fitting \", hhmm_results_name)\n",
    "            hhmm_results[hhmm_results_name] = fit(M, K, transitions, observations, hmm)\n",
    "\n",
    "with open(os.path.join(results_dir, \"hhmm_results.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(hhmm_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "for transitions in transitionss:\n",
    "    for observations in observationss:\n",
    "        # HMM\n",
    "        hmm_results_prefix = \"hmm_{}_{}_\".format(transitions, observations)\n",
    "        hmm_val_lls = [hmm_results[hmm_results_prefix + \"K{}\".format(K)][2] for K in Ks]\n",
    "        plt.plot(Ks, hmm_val_lls, '-o', label=hmm_results_prefix)\n",
    "\n",
    "        # HHMM\n",
    "        for eta in [1e-1]:\n",
    "            hhmm_results_prefix = \"hhmm_{}_{}_\".format(transitions, observations)\n",
    "            hhmm_val_lls = [hhmm_results[hhmm_results_prefix + \"K{}_eta{:1.0e}\".format(K, eta)][1][2] for K in Ks]\n",
    "            plt.plot(Ks, hhmm_val_lls, '-o', label=hhmm_results_prefix + \" eta={:1.0e}\".format(eta))\n",
    "            \n",
    "        # HHMM with inputs\n",
    "        hhmm_results_prefix = \"hhmm_{}_{}_\".format(transitions, observations)\n",
    "        hhmm_val_lls = [hhmm_results[hhmm_results_prefix + \"K{}_eta{:1.0e}_inputs\".format(K, best_eta)][1][2] for K in Ks]\n",
    "        plt.plot(Ks, hhmm_val_lls, '-o', label=hhmm_results_prefix + \" (inputs)\")\n",
    "\n",
    "plt.xlabel(\"K\")\n",
    "plt.xticks(Ks)\n",
    "plt.ylabel(\"Validation LL\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhhmm = hhmm_results[\"hhmm_recurrent_robust_ar_K12_eta1e-03\"][0]\n",
    "irhhmm = hhmm_results[\"hhmm_recurrent_robust_ar_K12_eta1e-03_inputs\"][0]\n",
    "\n",
    "print(\"rhHHMM val: \", rhhmm.log_likelihood(val_xs, tags=val_tags))\n",
    "print(\"irhHHMM val: \", irhhmm.log_likelihood(val_xs, inputs=val_inputs, tags=val_tags))\n",
    "print(\"rhHHMM test: \", rhhmm.log_likelihood(test_xs, tags=test_tags))\n",
    "print(\"irhHHMM test: \", irhhmm.log_likelihood(test_xs, inputs=test_inputs, tags=test_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort the states somehow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the inferred states\n",
    "# z_infs = [irhhmm.most_likely_states(x, input=u, tag=g) for g, x, u in zip(worms_and_groups, xs, all_inputs)]\n",
    "# # irhhmm.permute(find_permutation(np.concatenate(z_trues), np.concatenate(z_infs)))\n",
    "\n",
    "# # Permute the states based on the overlap with the manually labeled states\n",
    "# K_zimmer = len(z_true_key)\n",
    "# overlap = np.zeros((K_zimmer, irhhmm.K), dtype=float)\n",
    "# for ww in range(W):\n",
    "#     for k1 in range(K_zimmer):\n",
    "#         for k2 in range(irhhmm.K):\n",
    "#             overlap[k1, k2] += np.sum((z_trues[ww] == k1) & (z_infs[ww] == k2))\n",
    "\n",
    "# overlap /= overlap.sum(axis=1, keepdims=True)\n",
    "# best_match = np.argmax(overlap, axis=0)\n",
    "# perm = np.argsort(best_match)\n",
    "# irhhmm.permute(perm)\n",
    "\n",
    "# new_colors = [zplt.default_colors[best_match[k]] for k in perm]\n",
    "# new_colors = np.array(new_colors)\n",
    "# new_colors = np.column_stack((new_colors, np.ones(irhhmm.K)))\n",
    "# for k in range(1, irhhmm.K):\n",
    "#     if best_match[perm][k] == best_match[perm][k-1]:\n",
    "#         new_colors[k][3] = new_colors[k-1][3] * .8\n",
    "# from hips.plotting.colormaps import gradient_cmap\n",
    "# new_cmap = gradient_cmap(new_colors)\n",
    "\n",
    "# plt.imshow(overlap[:, perm])\n",
    "\n",
    "# z_infs = [irhhmm.most_likely_states(x, input=u, tag=g) for g, x, u in zip(worms_and_groups, xs, all_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the inferred states\n",
    "z_infs = [irhhmm.most_likely_states(x, input=u, tag=g) for g, x, u in zip(worms_and_groups, xs, all_inputs)]\n",
    "usage = np.bincount(np.concatenate(z_infs), minlength=irhhmm.K)\n",
    "perm = np.argsort(usage)[::-1]\n",
    "irhhmm.permute(perm)\n",
    "\n",
    "new_colors = zplt.default_colors\n",
    "new_cmap = zplt.default_cmap\n",
    "\n",
    "plt.imshow(overlap[:, perm])\n",
    "\n",
    "z_infs = [irhhmm.most_likely_states(x, input=u, tag=g) for g, x, u in zip(worms_and_groups, xs, all_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permute states based on how they respond to change in oxygen\n",
    "do2_on_effect = np.mean(irhhmm.transitions.Ws[:, :, 1] * 2.121, axis=0)\n",
    "perm = np.argsort(do2_on_effect)\n",
    "irhhmm.permute(perm)\n",
    "\n",
    "new_colors = zplt.default_colors\n",
    "new_cmap = zplt.default_cmap\n",
    "\n",
    "plt.imshow(overlap[:, perm])\n",
    "\n",
    "z_infs = [irhhmm.most_likely_states(x, input=u, tag=g) for g, x, u in zip(worms_and_groups, xs, all_inputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the overlap between inferred and true states\n",
    "K_zimmer = len(z_true_key)\n",
    "overlap = np.zeros((K_zimmer, irhhmm.K), dtype=float)\n",
    "for ww in range(W):\n",
    "    for k1 in range(K_zimmer):\n",
    "        for k2 in range(irhhmm.K):\n",
    "            overlap[k1, k2] += np.sum((z_trues[ww] == k1) & (z_infs[ww] == k2))\n",
    "\n",
    "# Normalize the overlap from all worms and plot\n",
    "overlap /= overlap.sum(1)[:, None]\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "gs = GridSpec(11, 12)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[:10, 1:11])\n",
    "im = ax1.imshow(overlap, vmin=0, vmax=1.0, cmap=\"Greys\", interpolation=\"nearest\", aspect=\"equal\")\n",
    "ax1.set_yticks([])\n",
    "ax1.set_xticks([])\n",
    "ax1.set_title(\"state overlap\")\n",
    "\n",
    "lax = fig.add_subplot(gs[:10, 0])\n",
    "lax.imshow(np.arange(K_zimmer)[:, None], cmap=zplt.default_cmap, \n",
    "              vmin=0, vmax=len(zplt.default_colors) - 1, aspect=1.5)\n",
    "\n",
    "lax.set_xticks([])\n",
    "lax.set_yticks(np.arange(K_zimmer))\n",
    "lax.set_yticklabels(z_true_key)\n",
    "\n",
    "bax = fig.add_subplot(gs[10, 1:11])\n",
    "bax.imshow(np.arange(irhhmm.K)[None, :], cmap=new_cmap,  aspect=\"auto\",\n",
    "           vmin=0, vmax=len(new_colors)-1)\n",
    "bax.set_xticks([])\n",
    "bax.set_yticks([])\n",
    "bax.set_xlabel(\"inferred state\")\n",
    "\n",
    "axcb = fig.add_subplot(gs[:10, 11])\n",
    "plt.colorbar(im, cax=axcb, ticks=[0, 0.5, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute state usage over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a mountain plot of state usage\n",
    "usage = np.zeros((len(groups), np.max(Ts), best_K))\n",
    "for g, group in enumerate(groups):\n",
    "    for w, (_, gg) in enumerate(worms_and_groups):\n",
    "        if gg != group:\n",
    "            continue\n",
    "        \n",
    "        zw = z_infs[w]\n",
    "        Tw = Ts[w]\n",
    "        usage[g * np.ones(Tw, dtype=int), np.arange(Tw), zw.astype(int)] += 1\n",
    "        \n",
    "avg_usage = usage / usage.sum(axis=-1, keepdims=True)\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "smooth_avg_usage = gaussian_filter1d(avg_usage, 3, axis=1)\n",
    "# smooth_avg_usage = avg_usage.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute oxygen effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2_effect = irhhmm.transitions.Ws[:, :, 0] * (1.4141176470588237 - -0.7073866923818708)\n",
    "do2_on_effect = irhhmm.transitions.Ws[:, :, 1] * np.max(all_inputs[0][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_figure_8(zs, tmin=1, tmax=17):\n",
    "    slc = slice(int(tmin*60*3), int(tmax*60*3+1))\n",
    "    tslc = np.arange(int(tmin*60*3), int(tmax*60*3+1)) / 3.0 / 60.\n",
    "\n",
    "    # Make plot\n",
    "    fig = plt.figure(figsize=(6.5, 5))\n",
    "    fig.patch.set_alpha(0)\n",
    "    gs = GridSpec(5, 5, height_ratios=[1, 4, 4, 4, 4])\n",
    "\n",
    "    # Plot the oxygen level\n",
    "    plt.subplot(gs[0, :3])\n",
    "    plt.imshow(us[0][None, :], aspect=\"auto\", vmin=np.min(us[0]), vmax=2*np.max(us[0]), cmap=\"Greys\")\n",
    "    plt.text(3 * 60 * 3, .3, \"10% O$_2$\", fontsize=8, horizontalalignment=\"center\")\n",
    "    plt.text(9 * 60 * 3, .3, \"21% O$_2$\", fontsize=8, horizontalalignment=\"center\")\n",
    "    plt.text(15 * 60 * 3, .3, \"10% O$_2$\", fontsize=8, horizontalalignment=\"center\")\n",
    "    plt.plot([6 * 60 * 3, 6 * 60 * 3], [-.5, .5], '-k', lw=2)\n",
    "    plt.plot([12 * 60 * 3, 12 * 60 * 3], [-.5, .5], '-k', lw=2)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    # Make a legend\n",
    "#     ax = plt.subplot(gs[0, 3:])\n",
    "#     ax.set_xticks([])\n",
    "#     ax.set_yticks([])    \n",
    "#     labels = ['Q', 'Fwd', 'Rev', 'VT', 'DT']\n",
    "#     for k in range(5):\n",
    "#         ax.bar(0, 0, width=0, color=zplt.default_colors[k], label=labels[k])\n",
    "#     plt.legend(ncol=5, loc=\"center\", handlelength=1, handletextpad=.5, columnspacing=1)\n",
    "    \n",
    "    for g, group in zip([0, 1, 2, 3], [\"N2\\n(prelethargic)\", \"N2\\n(lethargic)\", \"npr1\\n(prelethargic)\", \"npr1\\n(lethargic)\"]):\n",
    "        plt.subplot(gs[g + 1, :3])\n",
    "\n",
    "        # Make mountain plot\n",
    "        offset = np.zeros((np.max(Ts),))\n",
    "        for k in range(best_K):\n",
    "            # plt.fill_between(np.arange(np.max(Ts)), offset, offset + smooth_avg_usage[g, :, k], color=new_colors[k])\n",
    "            plt.fill_between(np.arange(np.max(Ts)), offset, offset + smooth_avg_usage[g, :, k], color=zplt.default_colors[k])\n",
    "            offset += smooth_avg_usage[g, :, k]\n",
    "\n",
    "        # Plot dividers\n",
    "        plt.plot([6 * 60 * 3, 6 * 60 * 3], [0, 1], '-k', lw=1)\n",
    "        plt.plot([12 * 60 * 3, 12 * 60 * 3], [0, 1], '-k', lw=1)\n",
    "\n",
    "        plt.xlim(0, np.max(Ts)+1)\n",
    "        if g < 3:\n",
    "            plt.xticks([])\n",
    "        else:\n",
    "            plt.xticks(np.arange(18) * 60 * 3, np.arange(18))\n",
    "            plt.xlabel(\"time (min)\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.yticks([0, 1])\n",
    "        plt.ylabel(group)\n",
    "        \n",
    "        # Plot the oxygen effect\n",
    "        plt.subplot(gs[g+1, 3])\n",
    "        for k in range(irhhmm.K):\n",
    "            # plt.bar(k, o2_effect[g, k], width=0.8, color=new_colors[k])\n",
    "            plt.bar(k, o2_effect[g, k], width=0.8, color=zplt.default_colors[k])\n",
    "        plt.plot([-1, irhhmm.K], [0, 0], '-k')\n",
    "        plt.xlim(-.5, irhhmm.K-.5)\n",
    "        plt.xticks([])\n",
    "        plt.ylim(-1.5, 1.5)\n",
    "        # plt.ylim(-6, 6)\n",
    "        if g == 0:\n",
    "            plt.title(\"O$_2$ effect\", y=0.95)\n",
    "        if g == 3:\n",
    "            plt.xlabel(\"state\")\n",
    "\n",
    "        # Plot the change in oxygen effect\n",
    "        plt.subplot(gs[g+1, 4])\n",
    "        for k in range(irhhmm.K):\n",
    "            # plt.bar(k, do2_on_effect[g, k], width=0.8, color=new_colors[k])\n",
    "            plt.bar(k, do2_on_effect[g, k], width=0.8, color=zplt.default_colors[k])\n",
    "        plt.plot([-1, irhhmm.K], [0, 0], '-k')\n",
    "        plt.xlim(-.5, irhhmm.K-.5)\n",
    "        plt.xticks([])\n",
    "        plt.ylim(-.75, .75)\n",
    "        if g == 0:\n",
    "            plt.title(\"$\\Delta$O$_2$ effect\", y=0.95)\n",
    "        if g == 3:\n",
    "            plt.xlabel(\"state\")\n",
    "            \n",
    "        plt.tight_layout(pad=0.25)\n",
    "\n",
    "    figname = \"8_inputs\"\n",
    "    plt.savefig(os.path.join(results_dir, \"figures\", figname + \".pdf\"))\n",
    "    plt.savefig(os.path.join(results_dir, \"figures\", figname + \".png\"), dpi=300)\n",
    "        \n",
    "make_figure_8(z_infs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple decoding analysis\n",
    "\n",
    "For each worm, measure the likelihood of its activity under the average parameters of worms from each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average transition paramters for each group\n",
    "log_Ps = irhhmm.transitions.shared_log_Ps\n",
    "Rs = irhhmm.transitions.shared_Rs\n",
    "Ws = irhhmm.transitions.Ws\n",
    "\n",
    "# Get the average observation parameters\n",
    "mu_init = irhhmm.observations.mu_init\n",
    "inv_sigma_init = irhhmm.observations.inv_sigma_init\n",
    "As = irhhmm.observations.shared_As\n",
    "bs = irhhmm.observations.shared_bs\n",
    "Vs = irhhmm.observations.shared_Vs\n",
    "inv_sigmas = irhhmm.observations.inv_sigmas.mean(0)\n",
    "inv_nus = irhhmm.observations.inv_nus.mean(0)\n",
    "\n",
    "# Construct a test model \n",
    "decoded_ll = np.zeros((W, 4))\n",
    "for w in range(W):\n",
    "    for g in range(4):\n",
    "        init_state_distn = copy.deepcopy(irhhmm.init_state_distn)\n",
    "\n",
    "        transition_distn = RecurrentTransitions(irhhmm.K, D, M=M)\n",
    "        transition_distn.log_Ps = log_Ps.copy()\n",
    "        transition_distn.Rs = Rs.copy()\n",
    "        transition_distn.Ws = Ws[g].copy()\n",
    "\n",
    "        observation_distn = RobustAutoRegressiveObservations(irhhmm.K, D, M=M)\n",
    "        observation_distn.mu_init = mu_init.copy()\n",
    "        observation_distn.inv_sigma_init = inv_sigma_init.copy()\n",
    "        observation_distn.As = As.copy()\n",
    "        observation_distn.bs = bs.copy()\n",
    "        observation_distn.Vs = Vs.copy()\n",
    "        observation_distn.inv_sigmas = inv_sigmas.copy()\n",
    "        observation_distn.inv_nus = inv_nus.copy()\n",
    "\n",
    "        test_hmm = _HMM(irhhmm.K, D, M, init_state_distn, transition_distn, observation_distn)\n",
    "        decoded_ll[w, g] = test_hmm.log_likelihood(xs[w], inputs=all_inputs[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the group labels\n",
    "group_sizes = [11, 12, 10, 11]\n",
    "\n",
    "# Normalize the decoded log likelihoods\n",
    "from scipy.special import logsumexp\n",
    "decoded_p = np.exp(decoded_ll - logsumexp(decoded_ll, axis=1, keepdims=True))\n",
    "plt.imshow(decoded_p, aspect=\"auto\", cmap=\"Greys\")\n",
    "\n",
    "for offset in np.cumsum(group_sizes)[:-1]:\n",
    "    plt.plot([-.5, 3.5], [offset-.5, offset-.5], '-r', lw=2)\n",
    "\n",
    "plt.xlabel(\"Group\")\n",
    "plt.ylabel(\"Worm\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a decoding confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_labels = np.repeat(np.arange(4), group_sizes)\n",
    "confusion = np.zeros((4, 4))\n",
    "for true_label, dec_label in zip(group_labels, np.argmax(decoded_p, axis=1)):\n",
    "    confusion[true_label, dec_label] += 1\n",
    "    \n",
    "confusion /= confusion.sum(axis=1, keepdims=True)\n",
    "    \n",
    "plt.imshow(confusion, vmin=0, cmap=\"Greys\")\n",
    "plt.xlabel(\"decoded group\")\n",
    "plt.xticks(np.arange(4), nice_groups)\n",
    "plt.ylabel(\"true group\")\n",
    "plt.yticks(np.arange(4), nice_groups)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try predicting neural activity in response to oxygen stimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 34\n",
    "T_pre = 6 * 60 * 3\n",
    "T_smpl = 1 * 60 * 3\n",
    "z_pre = z_infs[w][:T_pre]\n",
    "x_pre = xs[w][:T_pre]\n",
    "\n",
    "N_smpls = 100\n",
    "x_smpls = []\n",
    "for smpl in tqdm(range(N_smpls)):\n",
    "    z_smpl, x_smpl = irhhmm.sample(\n",
    "        T_smpl, prefix=(z_pre, x_pre), \n",
    "        input=all_inputs[w][:T_pre+T_smpl], \n",
    "        tag=worms_and_groups[w])\n",
    "    x_smpls.append(x_smpl)\n",
    "    \n",
    "x_mean = np.mean(x_smpls, axis=0)\n",
    "x_std  = np.std(x_smpls, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.vstack((x_pre, x_smpl)) - np.arange(D), '-k')\n",
    "plt.plot(xs[w][:T_pre+T_smpl] - 3 * np.arange(D), '-k', lw=2)\n",
    "for d in range(D):\n",
    "    plt.fill_between(np.arange(T_pre, T_pre + T_smpl), \n",
    "                     x_mean[:,d]-2*x_std[:,d]-3*d, \n",
    "                     x_mean[:,d]+2*x_std[:,d]-3*d, \n",
    "                     color='r', alpha=0.25)\n",
    "plt.plot(np.arange(T_pre, T_pre + T_smpl), np.mean(x_smpls, axis=0) - 3 * np.arange(D), '-r', lw=2)\n",
    "plt.plot([T_pre, T_pre], [-3 * D, 1], ':k')\n",
    "# plt.plot(np.arange(10, 10 + T_pre+T_smpl), -25 + 5 * all_inputs[w][:T_pre+T_smpl], lw=2)\n",
    "plt.xlim(T_pre-100, T_pre+T_smpl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
